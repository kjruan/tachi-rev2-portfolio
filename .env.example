# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Choose your LLM provider: ollama, openrouter, groq, or claude
# Default: ollama (local/free)
LLM_PROVIDER=ollama

# ----------------------------------------------------------------------------
# Ollama Configuration (Local/Free - Recommended for development)
# ----------------------------------------------------------------------------
# Install from: https://ollama.ai
# Models are downloaded automatically when first used
# Default models: qwen2.5:14b (strategist), llama3.2 (analyst/fetcher)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_API_BASE=http://localhost:11434  # For LiteLLM/CrewAI compatibility
OLLAMA_TIMEOUT=120

# Optional: Override default Ollama models
# STRATEGIST_MODEL=qwen2.5:14b
# ANALYST_MODEL=llama3.2:latest
# FETCHER_MODEL=llama3.2:latest

# ----------------------------------------------------------------------------
# OpenRouter Configuration (API Key Required - Free Tier Available)
# ----------------------------------------------------------------------------
# Get free API key from: https://openrouter.ai/keys
# Free tier: meta-llama models (3.2-1b, 3.2-3b)
# OPENROUTER_API_KEY=your_openrouter_key_here
# OPENROUTER_SITE_URL=https://github.com
# OPENROUTER_APP_NAME=rev2-portfolio

# Optional: Override default OpenRouter models
# STRATEGIST_MODEL=meta-llama/llama-3.2-3b-instruct:free
# ANALYST_MODEL=meta-llama/llama-3.2-3b-instruct:free
# FETCHER_MODEL=meta-llama/llama-3.2-1b-instruct:free

# ----------------------------------------------------------------------------
# Groq Configuration (API Key Required - Free Tier Available)
# ----------------------------------------------------------------------------
# Get free API key from: https://console.groq.com/keys
# Free tier: 30 requests/minute, generous daily limits
# GROQ_API_KEY=your_groq_key_here

# Optional: Override default Groq models
# STRATEGIST_MODEL=llama-3.3-70b-versatile
# ANALYST_MODEL=llama-3.1-70b-versatile
# FETCHER_MODEL=llama-3.1-8b-instant

# ----------------------------------------------------------------------------
# Claude API Configuration (Paid - Most Capable)
# ----------------------------------------------------------------------------
# Get API key from: https://console.anthropic.com/
# Note: Requires credits ($5 minimum)
# ANTHROPIC_API_KEY=your_api_key_here

# Optional: Override default Claude models
# STRATEGIST_MODEL=claude-sonnet-4-20250514
# ANALYST_MODEL=claude-sonnet-4-20250514
# FETCHER_MODEL=claude-3-5-haiku-20241022

# ============================================================================
# MCP (Model Context Protocol) Configuration
# ============================================================================
# Enable MCP servers for enhanced tool capabilities
# Leave commented/false unless you have specific MCP servers configured

# MCP GitHub Integration (requires GitHub token)
# GITHUB_TOKEN=your_github_token

# MCP Brave Search (requires Brave API key)
# BRAVE_API_KEY=your_brave_api_key

# ============================================================================
# Analysis Configuration
# ============================================================================
# Timeout for analysis tasks (seconds)
ANALYSIS_TIMEOUT=300

# Number of retries on failure
MAX_RETRIES=3

# Enable verbose output (true/false)
VERBOSE=false

# Enable agent memory (true/false)
ENABLE_MEMORY=true

# Maximum concurrent analyses
MAX_CONCURRENT=3

# ============================================================================
# API Server Configuration
# ============================================================================
# API server for tachi integration
API_HOST=0.0.0.0
API_PORT=8001
DEBUG=false

# ============================================================================
# Logging Configuration
# ============================================================================
LOG_LEVEL=INFO
